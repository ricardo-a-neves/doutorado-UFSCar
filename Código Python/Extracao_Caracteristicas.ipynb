{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicação de técnicas do Método Proposto\n",
    "---\n",
    "## Extração de Características"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3090,
     "status": "ok",
     "timestamp": 1628273779130,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "dl_UZFzNJrcN"
   },
   "outputs": [],
   "source": [
    "# Bibliotecas Utilizadas\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndi\n",
    "from sklearn.cluster import KMeans \n",
    "import skimage.color\n",
    "import cv2\n",
    "from scipy import misc\n",
    "from skimage.color import rgb2gray\n",
    "from skimage import color\n",
    "import skimage.metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "import functions as fun\n",
    "import time\n",
    "from skimage.feature import hog\n",
    "from skimage import exposure\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from math import copysign, log10\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.impute import KNNImputer\n",
    "from os import chdir, getcwd, listdir\n",
    "from os.path import isfile\n",
    "import os\n",
    "import oci\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurações e funcionalidades dos Buckets: Oracle Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para configurações da Cloud\n",
    "def config_cloud(flag):\n",
    "\n",
    "    # Definição de variável URL de acesso ao Endpoint Oracle Cloud\n",
    "    if flag == 'p':\n",
    "        # Ambiente de Produção - Config Cloud\n",
    "        url = \"https://grinqewrovfi.compat.objectstorage.sa-saopaulo-1.oraclecloud.com\" \n",
    "    \n",
    "    elif flag =='t':\n",
    "        \n",
    "        # Ambiente de Testes - Config Cloud\n",
    "        url = \"https://grwbzp0j0zza.compat.objectstorage.sa-saopaulo-1.oraclecloud.com\"\n",
    "\n",
    "    # Chamada de função para obter as configurações da Cloud\n",
    "    fun.configuration_Cloud()\n",
    "\n",
    "    # Chamada de função para obter o resource\n",
    "    resource_value = fun.resource()\n",
    "       \n",
    "    return url, resource_value\n",
    "\n",
    "# Chamada de função para configurar a Cloud \n",
    "url, resource_value = config_cloud('p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurações para Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Armazenar todos os nomes dos arquivos de imagens do Banco de Imagens\n",
    "\n",
    "# Definição dos \"Paths\"\n",
    "\n",
    "full_path = \"/home/datascience/classes-images-selected/\"\n",
    "path = \"/home/datascience/classes-images-selected/\"\n",
    "\n",
    "# Definição do contador para automatizar\n",
    "contador_imagens = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para pegar os objetos selecionados no bucket\n",
    "def objects_to_bucket(flag, bucket, resource_value, url, contador_imagens):\n",
    "            \n",
    "    if os.path.exists(path) ==False:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    # Imprimir os buckets encontrados\n",
    "    try:\n",
    "        for bucket_name in resource_value.buckets.all():\n",
    "            print (bucket_name.name)\n",
    "        print(\"\\n Conexao com sucesso na cloud!!\\n\")\n",
    "    except:\n",
    "        print(\"Falha de conexao na cloud\")\n",
    "    \n",
    "    files = fun.getObjectsToBucket(str(bucket), resource_value)\n",
    "\n",
    "    # Flag 'd' - Processo de download de dados via (API) Boto3\n",
    "    if flag == 'd':\n",
    "        # Chamada de funções para upload de arquivo: destino 'pasta de trabalho do ambiente data science'\n",
    "        fun.download_to_bucket(str(bucket), url, files, path)\n",
    "        message = \"Download de arquivos com sucesso!!\"\n",
    "    \n",
    "    # Flag 's' - Processo de sincronismo de dados via (API) rclone\n",
    "    elif flag == 's':\n",
    "        try:\n",
    "            #rclone sync source:path dest:path\n",
    "            cmd = 'rclone sync ' + str(bucket) + ':' + str(bucket) + ' ' + path \n",
    "            os.system(cmd)\n",
    "            message = \"Arquivos sincronizados com successo!!\"\n",
    "        except:\n",
    "            message = \"Erro de sincronismo\"\n",
    "    \n",
    "    return message, path\n",
    "\n",
    "# Chamada de função para obter objetos selecionados do bucket\n",
    "message = objects_to_bucket('s', 'BK-feature-selected', resource_value, url, contador_imagens)\n",
    "print(message)\n",
    "\n",
    "# Armazenar os arquivos de entrada para serem processados\n",
    "filepaths = [f for f in os.listdir(path) if f.endswith('.jpg')]\n",
    "\n",
    "# Imprimir o resultado dos arquivos processados\n",
    "print(\"\\n Quantidade de Arquivos Processados: \" + str(len(filepaths)))\n",
    "\n",
    "# Imprimir a posição de cada arquivo (Facilitar a localização em caso de falha de processamento)\n",
    "for i in range(0,len(filepaths)):\n",
    "    print(\"\\n\" + str(i) + \" --> \" + filepaths[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para Seleção automática de imagens no vetor carregado\n",
    "def selecao_automatica_img(filepaths, contador_imagens):\n",
    "    nome_imagem = '/' + filepaths[contador_imagens]     \n",
    "\n",
    "    # Verifica a \"classe\" da imagem selecionada (Classe Verde = 1 | Classe Amarela = 2 | Classe Marrom = 3)\n",
    "    if 'verde' in nome_imagem:\n",
    "        cor = 'verde'\n",
    "        op = 1\n",
    "    elif 'amarela' in nome_imagem:\n",
    "        cor = 'amarela'\n",
    "        op = 2\n",
    "    elif 'marrom' in nome_imagem:\n",
    "        cor = 'marrom'\n",
    "        op= 3\n",
    "        \n",
    "    # Deninir o nome da pasta de acordo com a imagem \n",
    "    pasta = nome_imagem[-12::]\n",
    "\n",
    "    path_out = 'caracteristicas_2/' + pasta + '/' + cor\n",
    "    # Verifica se a pasta já foi criada. Em caso negativo - cria a pasta\n",
    "    if os.path.exists(path_out) == False:\n",
    "        os.makedirs(path_out) \n",
    "\n",
    "    # Exibição da imagem selecionada e da cor a ser processada\n",
    "    print('Imagem Selecionada: ' + nome_imagem)\n",
    "    print('Cor a ser processada: ' + cor)\n",
    "    print('Iteração de nº: ' + str(contador_imagens))\n",
    "\n",
    "    return op, nome_imagem, path_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gMH4SO1p3JtS"
   },
   "source": [
    "#### HOG - Histogram of Oriented Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "executionInfo": {
     "elapsed": 4861,
     "status": "ok",
     "timestamp": 1628273790997,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "57lDFT9_Lpq5",
    "outputId": "d7a0f4e4-9f10-435e-d321-7537f9601342"
   },
   "outputs": [],
   "source": [
    "# Função para entrada do arquivo para análise HOG\n",
    "def entrada_imagem(full_path, nome_imagem):\n",
    "    image = cv2.cvtColor(cv2.imread(full_path + nome_imagem), cv2.COLOR_BGR2RGB)\n",
    "    gray = color.rgb2gray(image)\n",
    "    plt.figure(figsize=[15, 5])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.title('Imagem Original')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(gray)\n",
    "    plt.title('Imagem Tons de Cinza')\n",
    "\n",
    "    return image, gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 36659,
     "status": "ok",
     "timestamp": 1628273836668,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "fW-YUJv81-VA"
   },
   "outputs": [],
   "source": [
    "# Versão com a biblioteca \"skimage.feature import hog\"\n",
    "# Função para Processamento HOG\n",
    "def processamento_HOG(gray):\n",
    "    fd, hog_image = hog(gray, orientations=8,\\\n",
    "                        pixels_per_cell=(8,8),\\\n",
    "                        cells_per_block=(8,8),\\\n",
    "                        visualize=True,\\\n",
    "                        feature_vector=True,\\\n",
    "                        multichannel= False,\\\n",
    "                        transform_sqrt=True)\n",
    "\n",
    "    return fd, hog_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "executionInfo": {
     "elapsed": 3989,
     "status": "ok",
     "timestamp": 1628273847517,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "GZW_Upk-rZiJ",
    "outputId": "d1b62ff0-b84d-40ea-c942-e707a221a829"
   },
   "outputs": [],
   "source": [
    "# Função para plotagem da imagem HOG\n",
    "def plot_imagem_HOG(image, hog_image):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25, 25), sharex=True, sharey=True)\n",
    "\n",
    "    ax1.axis('off')\n",
    "    ax1.imshow(image, cmap=plt.cm.gray)\n",
    "    ax1.set_title('Imagem de Entrada')\n",
    "\n",
    "    # Restruturar o histograma para melhor visualização\n",
    "    hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 20))\n",
    "\n",
    "    ax2.axis('off')\n",
    "    ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray_r)\n",
    "    ax2.set_title('Histogram of Oriented Gradients')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "PoQjF06Vt-z4"
   },
   "outputs": [],
   "source": [
    "# Função para armazenar o vetor de características HOG em disco\n",
    "def armazenar_vetor_HOG(fd):\n",
    "    text = ''\n",
    "    for i in range(0,fd.size):\n",
    "        if (fd[i]!=0):\n",
    "            text+= '{}'.format(fd[i])\n",
    "            text+= '\\n'\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gravar a String em arquivo\n",
    "def gravar_string_arquivo(path_out,contador_imagens, url):\n",
    "    with open(path_out  + '/exemploHOG' + '_' + str(contador_imagens) + '.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "    \n",
    "    # Chamada de função para updates de arquivos da 'pasta de trabalho' para o bucket selecionado       \n",
    "    fun.upload_from_bucket('BK-feature-vector', url, path_out  + '/exemploHOG' + '_' + str(contador_imagens) + '.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JexvipTNcUkj"
   },
   "source": [
    "#### Exibição dos descritores HOG - Dataframe de Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 5363,
     "status": "ok",
     "timestamp": 1628273097243,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "j4f5jg9GcUPr",
    "outputId": "d4f54d0e-523f-40d6-f0ff-0b0a62429de5"
   },
   "outputs": [],
   "source": [
    "# Função para carregar os dados CSV no Dataframe\n",
    "def carregar_dados_dataframe(path_out, contador_imagens):\n",
    "    df_hog = pd.read_csv(path_out + '/exemploHOG' + '_' + str(contador_imagens) + '.txt', sep = ',', header = None)\n",
    "    # Gerar o cabeçalho para os dados CSV\n",
    "    df_hog.columns =['hog']\n",
    "    # Exibição do Dataframe\n",
    "    df_hog\n",
    "\n",
    "    return df_hog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwF1JRcf4Mdc"
   },
   "source": [
    "#### SIFT (Scale-Invariant Feature Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "executionInfo": {
     "elapsed": 7806,
     "status": "ok",
     "timestamp": 1628273875446,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "1AgIu-HY4MH9",
    "outputId": "89c6e0ae-f967-45b4-c39b-6b73dfd28b12"
   },
   "outputs": [],
   "source": [
    "# Função para processamento do SIFT\n",
    "def processar_SIFT(full_path, url, path_out, contador_imagens, nome_imagem, image):\n",
    "    image = cv2.imread(full_path + nome_imagem)\n",
    "    gray= cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute (gray, None)\n",
    "    image= cv2.drawKeypoints(gray,kp,image)\n",
    "    image=cv2.drawKeypoints(gray,kp,image,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "    cv2.imwrite(path_out + '/sift_keypoints' + '_' + str(contador_imagens) + '.jpg',image)\n",
    "\n",
    "    # Chamada de função para updates de arquivos da 'pasta de trabalho' para o bucket selecionado \n",
    "    fun.upload_from_bucket('BK-feature-vector', url, path_out + '/sift_keypoints' + '_' + str(contador_imagens) + '.jpg')\n",
    "\n",
    "    img_sift = cv2.imread(path_out + '/sift_keypoints' + '_' + str(contador_imagens) + '.jpg')\n",
    "    plt.figure(figsize=[15,15])\n",
    "    plt.title('Pontos Chave - SIFT')\n",
    "    plt.imshow(img_sift)\n",
    "\n",
    "    return kp, des, sift, img_sift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 375,
     "status": "ok",
     "timestamp": 1628273883561,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "x55G5mMv0yAg",
    "outputId": "4a8f42c0-b3cc-4490-9882-917274c80f7d"
   },
   "outputs": [],
   "source": [
    "# função para exibição dos características dados gerados SIFT\n",
    "def exibir_caracateristicas_dados_SIFT():\n",
    "    print(\" Tamanho:\" + str(des.size)+\",\\n Shape:\" + str(des.shape) + \",\\n Tipo:\" + str(des.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 40417,
     "status": "ok",
     "timestamp": 1628273148017,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "70QkpDvTzG3h",
    "outputId": "19a39dae-fd2e-4aa2-eac0-0cec55064d83"
   },
   "outputs": [],
   "source": [
    "# Função para armazenar o vetor de características SIFT em disco\n",
    "def armazenar_vetor_SIFT():\n",
    "    # Chamada da função para gravar matriz em arquivo CSV\n",
    "    fun.salvarMatrizArquivoCSV(des,path_out + '/exemploVetorSIFT' + '_' + str(contador_imagens) + '.csv')\n",
    "    ## Chamada de função para updates de arquivos da 'pasta de trabalho' para o bucket selecionado \n",
    "    fun.upload_from_bucket('BK-feature-vector', url, path_out + '/exemploVetorSIFT' + '_' + str(contador_imagens) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IR61CEaRdtT7"
   },
   "source": [
    "#### Exibição dos descritores SIFT - Dataframe de Dados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 439
    },
    "executionInfo": {
     "elapsed": 15869,
     "status": "ok",
     "timestamp": 1628273913643,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "hJTdReQRdtC8",
    "outputId": "5b693424-e250-4fdd-f3d7-91e93caeaeff"
   },
   "outputs": [],
   "source": [
    "# Função para gerar o cabeçalho para os dados CSV\n",
    "def gerar_data_header(des, path_out, contador_imagens):\n",
    "    header_list = []\n",
    "    lin, col = des.shape\n",
    "    for i in range(0,col):\n",
    "        header_list.append(\"SIFT\"+str([i]))\n",
    "        # Carrega os dados CSV no Dataframe\n",
    "        df_sift = pd.read_csv(path_out + '/exemploVetorSIFT' + '_' + str(contador_imagens) + '.csv', sep = ',',header= None)\n",
    "        \n",
    "    # Carrega os dados de cabeçalho no Dataframe\n",
    "    df_sift.columns = header_list\n",
    "    #Exibição do Dataframe\n",
    "    df_sift\n",
    "\n",
    "    return df_sift "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQCOl0OqJjNA"
   },
   "source": [
    "#### Momentos Invariantes de Hu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "gHYcxMfdJi_Q"
   },
   "outputs": [],
   "source": [
    "# Função para processar Momentos Invariantes de Hu\n",
    "def processar_momentos_HU(path_out, full_path, url, contador_imagens, nome_imagem):\n",
    "\n",
    "    # Ler a imagem\n",
    "    image = cv2.imread(full_path + nome_imagem, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Threshold image\n",
    "    _,image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Calcular os Momentos\n",
    "    moments = cv2.moments(image)\n",
    "    # Calcular os Momentos de Hu\n",
    "    huMoments = cv2.HuMoments(moments)\n",
    "    text_value=''\n",
    "    \n",
    "    for i in range(0,7):\n",
    "        huMoments[i] = -1* copysign(1.0, huMoments[i]) * log10(abs(huMoments[i]))\n",
    "        text_value += str(huMoments[i]) + '\\n'\n",
    "    \n",
    "    # Grava a String em arquivo e Exibe os valores calculados\n",
    "    with open(path_out + '/exemploMomentsHU' + '_' + str(contador_imagens) + '.txt', 'w') as f:\n",
    "        f.write(text_value)\n",
    "    # Chamada de função para updates de arquivos da 'pasta de trabalho' para o bucket selecionado \n",
    "    fun.upload_from_bucket('BK-feature-vector', url, path_out + '/exemploMomentsHU' + '_' + str(contador_imagens) + '.txt')\n",
    "\n",
    "    return moments, huMoments, text_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gerar o Dataframe Momentos HU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para tratar os dados dos Momentos HU e gerar o Dataframe\n",
    "def tratar_dados_HU_dataframe(path_out, contador_imagens):\n",
    "    df_hu = pd.read_csv(path_out + '/exemploMomentsHU' + '_' + str(contador_imagens) + '.txt', header= None)\n",
    "    # Carrega os dados de cabeçalho no Dataframe\n",
    "    df_hu.columns =['HU']\n",
    "    # Tratamento dos dados\n",
    "    df_hu['HU'] = df_hu['HU'].apply(lambda x: str(x).replace(\"[\",\"\")) # Retirada do \"[\" da string\n",
    "    df_hu['HU'] = df_hu['HU'].apply(lambda x: str(x).replace(\"]\",\"\")) # Retirada do \"]\" da string\n",
    "    # Conversão de string para Float\n",
    "    df_hu['HU'] = df_hu['HU'].astype('float64')\n",
    "    #Exibição dos dados Dataframe HU\n",
    "    df_hu\n",
    "\n",
    "    return df_hu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenação dos vetores de características (HOG, SIFT e HU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para concatenar os dados dos dataframes de características HOG, SIFT e HU\n",
    "def concatenar_dados_dataframes(df_hog, df_sift, df_hu):\n",
    "    df_total = pd.concat([df_hog, df_sift, df_hu], axis=1)\n",
    "\n",
    "    # Imprimir os dados na tela do conjunto total de dados\n",
    "    df_total\n",
    "\n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar arquivo CSV com os dados completos\n",
    "def gerar_arquivo_dados_completos(url, path_out,contador_imagens):\n",
    "    df_total.to_csv(path_out + '/dados_HOG_SIFT_HU' + '_' + str(contador_imagens) + '.csv', index = None, header=True)\n",
    "\n",
    "    # Chamada de função para updates de arquivos da 'pasta de trabalho' para o bucket selecionado \n",
    "    fun.upload_from_bucket('BK-feature-vector', url, path_out + '/dados_HOG_SIFT_HU' + '_' + str(contador_imagens) + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verificação de valores faltantes (Missing Values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para verificar se há colunas com valores faltantes no conjunto de dados\n",
    "def verificar_valores_faltantes():\n",
    "    df_total.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tratamento dos valores faltantes com '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para preencher os dados com valores \"0\"\n",
    "def preencher_dados_faltantes(df_total):\n",
    "    df_total['HU'] = df_total['HU'].fillna(0).head(9760000) # Coluna correspondente aos Momentos de HU\n",
    "    for i in range(0, 128):\n",
    "        nome = \"SIFT\" + str([i])\n",
    "        df_total[nome] = df_total[nome].fillna(0).head(9760000) # Colunas correspondentes ao SIFT\n",
    "    \n",
    "    return df_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para conferir se há valores faltantes no conjunto de dados - Pós tratamento\n",
    "def conferir_valores_faltantes():\n",
    "    df_total.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalização dos dados completos de caracterisiticas - Preparação para entrada no PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para normalização dos dados (HOG, SIFT e HU)\n",
    "def normalizar_dados(df_total):\n",
    "    normalized_df_total= (df_total-df_total.min())/(df_total.max()-df_total.min())\n",
    "    return normalized_df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlação dos dados - Preparação para entrada no PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar a correlação dos dados normalizados\n",
    "def aplicar_correlacao(normalized_df_total):\n",
    "    corr_df_total = normalized_df_total.corr()\n",
    "    corr_df_total\n",
    "    return corr_df_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise dos dados correlacionados - Gráfico de Dispersão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem do gráfico de dispersão dos dados correlacionados\n",
    "#sns.distplot(corr_df_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfico de HeatMap dos dados correlacionados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem do gráfico HeatMAp\n",
    "#plt.figure(figsize=[100,100])\n",
    "#sns.heatmap(corr_df_total, annot= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aplicação de PCA para redução de dimensionalidade - Vetor completo de Características (HOG, SIFT e HU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para aplicar PCA - Redução de Dimensionalidade\n",
    "def aplicar_PCA(normalized_df_total): #corr_df_total (trocado por normalized_df_total)\n",
    "    # Normaliza medidas, de forma que cada coluna de measurements_norm\n",
    "    # possui média 0 e desvio padrão 1\n",
    "    # measurements_norm = scale(normalized_df_total, axis=0) Retirado\n",
    "\n",
    "    # Cria instância da classe PCA, com projeção em 2 eixos (2D), e aplica\n",
    "    # o PCA nos dados\n",
    "    pca_instance = PCA(n_components=10)\n",
    "    pca_data = pca_instance.fit_transform(normalized_df_total)\n",
    "    return pca_instance, pca_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para exibir os dados do Dataframe PCA\n",
    "def exibir_dados_PCA(pca_data):\n",
    "    df_pca = pd.DataFrame(pca_data)\n",
    "    df_pca\n",
    "    return df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gráfico de HeatMap dos dados após aplicação do PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotagem do gráfico HeatMAp Após o PCA\n",
    "#plt.figure(figsize=[10,25])\n",
    "#sns.heatmap(df_pca, annot= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adicionar, nos dados, uma coluna com valor correspondente à Classe de Cor Processada. Recurso para uso na etapa de Análise de Dados - Via algoritmo de Machine Learning, onde: | 1: Classe Verde | 2: Classe Amarela | 3: Classe Marrom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para adicionar coluna no final do Dataframe com preenchimento de acordo com a Classe Processada\n",
    "# 1: Classe Verde | 2: Classe Amarela | 3: Classe Marrom\n",
    "def adicionar_coluna_dataframe(df_pca, op, path_out, nome_imagem, contador_imagens):\n",
    "    df_pca_export = df_pca\n",
    "    classe = []\n",
    "\n",
    "    for i in range(len(df_pca_export)):\n",
    "        classe.append(op)\n",
    "    # \n",
    "    df_pca_export['classe'] = classe\n",
    "\n",
    "    # Geração de arquivo CSV com os dados completos\n",
    "    df_pca_export.to_csv(path_out + nome_imagem + '_' + 'dados_PCA' + '_' + str(contador_imagens) + '.csv', index = None, header=True)\n",
    "    # Chamada de função para updates de arquivos da 'pasta de trabalho' para o bucket selecionado\n",
    "    fun.upload_from_bucket('BK-feature-vector', url, path_out + nome_imagem + '_' + 'dados_PCA' + '_' + str(contador_imagens) + '.csv')\n",
    "    df_pca_export\n",
    "\n",
    "    return df_pca_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada para execução do Processo de Extração de Características\n",
    "# Execução de loop de acordo com a quantidade de imagens para processamento\n",
    "\n",
    "for i in range(0,10): # range = quantidade de pastas disponíveis no bucket 'BK-feature-vector'\n",
    "    # Chamada de função para seleção automática de imagens\n",
    "    op, nome_imagem, path_out = selecao_automatica_img(filepaths, contador_imagens)\n",
    "\n",
    "    # Chamada de função para entrada do arquivo para análise HOG\n",
    "    image, gray = entrada_imagem(full_path, nome_imagem)\n",
    "\n",
    "    # Chamada de função para Processamento HOG\n",
    "    fd, hog_image = processamento_HOG(gray)\n",
    "\n",
    "    # Chamada de função para plotagem da imagem HOG\n",
    "    plot_imagem_HOG(image, hog_image)\n",
    "\n",
    "    # Chamada para armazenar o vetor de características HOG em disco\n",
    "    text = armazenar_vetor_HOG(fd)\n",
    "\n",
    "    # Chamada de função para gravar a String em arquivo\n",
    "    gravar_string_arquivo(path_out,contador_imagens, url)\n",
    "\n",
    "    # Chamada de função para carregar os dados CSV no Dataframe\n",
    "    df_hog = carregar_dados_dataframe(path_out, contador_imagens)\n",
    "\n",
    "    # Chamada de função para processamento do SIFT\n",
    "    kp, des, sift, img_sift = processar_SIFT(full_path, url, path_out, contador_imagens, nome_imagem, image)\n",
    "\n",
    "    # Chamada de função para exibição dos características dados gerados SIFT\n",
    "    exibir_caracateristicas_dados_SIFT()\n",
    "    \n",
    "    # Chamada de função para armazenar o vetor de características SIFT em disco\n",
    "    armazenar_vetor_SIFT()\n",
    "\n",
    "    # Chamada de função para gerar o cabeçalho para os dados CSV\n",
    "    df_sift = gerar_data_header(des, path_out, contador_imagens)\n",
    "\n",
    "    # Chamada de função para processar Momentos Invariantes de Hu\n",
    "    moments, huMoments, text_value = processar_momentos_HU(path_out, full_path, url, contador_imagens, nome_imagem)\n",
    "\n",
    "    # Chamada de função para tratar os dados dos Momentos HU e gerar o Dataframe\n",
    "    df_hu = tratar_dados_HU_dataframe(path_out, contador_imagens)\n",
    "\n",
    "    # Chamada de função para concatenar os dados dos dataframes de características HOG, SIFT e HU\n",
    "    df_total = concatenar_dados_dataframes(df_hog, df_sift, df_hu)\n",
    "\n",
    "    # Função para gerar arquivo CSV com os dados completos\n",
    "    gerar_arquivo_dados_completos(url, path_out,contador_imagens)\n",
    "\n",
    "    # Chamada de função para verificar se há colunas com valores faltantes no conjunto de dados\n",
    "    verificar_valores_faltantes()\n",
    "\n",
    "    # Chamada de função para preencher os dados com valores \"0\"\n",
    "    df_total = preencher_dados_faltantes(df_total)\n",
    "\n",
    "    # Chamada de função para conferir se há valores faltantes no conjunto de dados - Pós tratamento\n",
    "    conferir_valores_faltantes()\n",
    "\n",
    "    # Chamada de função para normalização dos dados (HOG, SIFT e HU)\n",
    "    normalized_df_total = normalizar_dados(df_total)\n",
    "\n",
    "    # Chamada de função para aplicar a correlação dos dados normalizados\n",
    "    corr_df_total = aplicar_correlacao(normalized_df_total)\n",
    "\n",
    "    # Chamada de função para aplicar PCA - Redução de Dimensionalidade\n",
    "    pca_instance, pca_data = aplicar_PCA(corr_df_total)\n",
    "\n",
    "    # Chamada de função para exibir os dados do Dataframe PCA\n",
    "    df_pca = exibir_dados_PCA(pca_data)\n",
    "\n",
    "    # Chamada de função para adicionar coluna no final do Dataframe com preenchimento de acordo com a Classe Processada\n",
    "    df_pca_export = adicionar_coluna_dataframe(df_pca, op, path_out, nome_imagem, contador_imagens)\n",
    "\n",
    "    # Incremento do contador de pastas\n",
    "    contador_imagens+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMsdK7xRb5p/d9P1lgT0Cft",
   "collapsed_sections": [],
   "name": "Qualificação - Extração de Características.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "e31a0e611b8724fca1d58a7ef46067b50a018b39ecc580ed645151c3e5f1bc5d"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
