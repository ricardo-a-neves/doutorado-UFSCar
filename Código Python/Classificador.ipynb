{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er2txkP9lVrj"
   },
   "source": [
    "#### Classificador Binário - Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {
    "executionInfo": {
     "elapsed": 1683,
     "status": "ok",
     "timestamp": 1608133339484,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "fKB6A242lM17"
   },
   "outputs": [],
   "source": [
    "# Bibliotecas Utilizadas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import warnings\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from itertools import cycle\n",
    "from scipy import interp\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "import IPython\n",
    "from IPython.display import display, Javascript\n",
    "from docx2pdf import convert\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor\n",
    "from docx.shared import Inches\n",
    "import functions as fun\n",
    "import oci\n",
    "import boto3\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Ignorar os avisos de Warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configurações e funcionalidades dos Buckets: Oracle Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para configurações da Cloud\n",
    "def config_cloud(flag):\n",
    "\n",
    "    # Definição de variável URL de acesso ao Endpoint Oracle Cloud\n",
    "    if flag == 'p':\n",
    "        # Ambiente de Produção - Config Cloud\n",
    "        url = \"https://grinqewrovfi.compat.objectstorage.sa-saopaulo-1.oraclecloud.com\" \n",
    "    \n",
    "    elif flag =='t':\n",
    "        \n",
    "        # Ambiente de Testes - Config Cloud\n",
    "        url = \"https://grwbzp0j0zza.compat.objectstorage.sa-saopaulo-1.oraclecloud.com\"\n",
    "\n",
    "    # Chamada de função para obter as configurações da Cloud\n",
    "    fun.configuration_Cloud()\n",
    "\n",
    "    # Chamada de função para obter o resource\n",
    "    resource_value = fun.resource()\n",
    "       \n",
    "    return url, resource_value\n",
    "\n",
    "# Chamada de função para configurar a Cloud \n",
    "url, resource_value = config_cloud('p')\n",
    "\n",
    "# Print all buckets\n",
    "for bucket in resource_value.buckets.all():\n",
    "    print (bucket.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração de Variáveis - Inicialização\n",
    "contador_pastas = 0\n",
    "\n",
    "# Parâmetro (Porcentagem de dados para Teste)\n",
    "teste = .2\n",
    "\n",
    "# Definição do path de entrada de dados para Treinamento e Teste do Modelo\n",
    "path_1 = 'aprendizado_maquinas/processar_1/'\n",
    "\n",
    "# Definição do path de entrada de dados para Teste de Novos Dados\n",
    "path_2 = 'aprendizado_maquinas/processar_2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para pegar os objetos selecionados no bucket\n",
    "def objects_to_bucket(etapa, flag, bucket, resource_value, url, path):\n",
    "            \n",
    "   # Caminho da pasta (principal) de entrada dos dados\n",
    "    if os.path.exists(path)==False:\n",
    "        os.makedirs(path)\n",
    "        \n",
    "    # Imprimir os buckets encontrados\n",
    "    try:\n",
    "        for bucket_name in resource_value.buckets.all():\n",
    "            print (bucket_name.name)\n",
    "        print(\"\\n Conexao com sucesso na cloud!!\\n\")\n",
    "    except:\n",
    "        print(\"Falha de conexao na cloud\")\n",
    "    \n",
    "    files = fun.getObjectsToBucket(str(bucket), resource_value)\n",
    "\n",
    "    # Flag 'd' - Processo de download de dados via (API) Boto3\n",
    "    if flag == 'd':\n",
    "        # Function call for download file: destination data science workspace - folder db_imagens\n",
    "        fun.download_to_bucket(str(bucket), url, files, path)\n",
    "        message = \"Download de arquivos com sucesso!!\"\n",
    "    \n",
    "    # Flag 's' - Processo de sincronismo de dados via (API) rclone\n",
    "    elif flag == 's':\n",
    "        try:\n",
    "            #rclone sync source:path dest:path\n",
    "            cmd = 'rclone sync ' + str(bucket) + ':' + str(bucket) + ' ' + path \n",
    "            os.system(cmd)\n",
    "            message = \"Arquivos sincronizados com successo!!\"\n",
    "        except:\n",
    "            message = \"Erro de sincronismo\"\n",
    "    \n",
    "    return message, etapa\n",
    "\n",
    "# Chamada de função para obter objetos selecionados do bucket\n",
    "message, etapa = objects_to_bucket(1,'s', 'BK-input-data-classification-full', resource_value, url, path_1)\n",
    "print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotinas de definição para estruturação dos arquivos\n",
    "\n",
    "def files_structure(contador_pastas, path):\n",
    "    \n",
    "    #------- loop de pastas (monta a estrutura e lê os arquivos em cada caso:) \n",
    "    #--------Caso 1: Treinamento e Teste do Classificador=> Lê o bucket com todas as características das imagens\n",
    "    #--------Caso 2: Nova Predição -> Depois do classificador treinado=> Lê o bucket que tem as características \n",
    "    #--------------------------------------------------------------------separadas por imagens dentro de sua pasta.\n",
    "\n",
    "    # Mostrar a iteração do loop\n",
    "    print(\"Iteração Nº: \", contador_pastas)\n",
    "\n",
    "    # Listar as subpastas dentro da pasta principal (retira arquivo de controle de versão - Git)\n",
    "    filepaths_1 = [f for f in os.listdir(path) if not f.endswith('.ipynb_checkpoints')]\n",
    "\n",
    "    # Acesso à pasta a ser trabalhada (no loop), uma por vez, conforme o \"contador _pastas\"\n",
    "    path_local = path + filepaths_1[contador_pastas]\n",
    "\n",
    "    if path == path_1:\n",
    "        # Definição da pasta de saída do processamento, uma por vez, conforme o \"contador _pastas\"\n",
    "        path_out = 'aprendizado_maquinas/output_1/' + filepaths_1[contador_pastas] + '/'\n",
    "    \n",
    "    elif path == path_2:\n",
    "        # Definição da pasta de saída do processamento, uma por vez, conforme o \"contador _pastas\"\n",
    "        path_out = 'aprendizado_maquinas/output_2/' + filepaths_1[contador_pastas] + '/'\n",
    "\n",
    "    # Criar a pasta \"output\" de acordo com a imagem de trabalho (na iteração atual do loop)\n",
    "    # Criar uma estrutura espelho de pastas de \"saída\", de acordo com a estrutura da pasta de \"entrada\"\n",
    "    os.makedirs(path_out)\n",
    "\n",
    "    # Estrutura que armazena o \"path\"  (atual) contido em cada pasta a ser trabalhada (no loop)\n",
    "    filepaths = [f for f in os.listdir(path_local) if f.endswith('.csv')]\n",
    "    # A partir do path (incompleto) monta o path (completo)\n",
    "    filepaths = [os.path.join(path_local, name) for name in filepaths]\n",
    "    print(filepaths)\n",
    "\n",
    "    #-------------------------------------------------------------------------------------\n",
    "\n",
    "    # A partir dos dados no formato (.csv) dos arquivos das 3 classes: Verde, Amarela e Marrom de cada imagem\n",
    "    #   unir os dados de caracaterísticas geradas para processamento do classificador\n",
    "    df=pd.DataFrame()\n",
    "    df = pd.concat(map(pd.read_csv, filepaths))\n",
    "    df.columns = ['A0','A1','A2','A3','A4','A5','A6','A7','A8','A9','C']\n",
    "    \n",
    "    gerarGraficos(df)\n",
    "\n",
    "    # Criar um dataframe para armazenamento dos dados para classificação binária\n",
    "    #   onde (0: significa não ter a doença -> 1: significa a presença da doença)\n",
    "    df_bin = pd.DataFrame()\n",
    "    df_bin = df.copy()\n",
    "\n",
    "    # Gravar os concatenados no dataframe (com as informações originais das 3 classes) em disco\n",
    "    df.to_csv(path_out + 'dados_PCA_' + filepaths_1[contador_pastas] + '_.csv', index = None, header=True)\n",
    "    # Function call Upload files function from workspace files to Selected Bucket\n",
    "    fun.upload_from_bucket('BK-classification', url, path_out + 'dados_PCA_' + filepaths_1[contador_pastas] + '_.csv')\n",
    "        \n",
    "    return df_bin, filepaths_1, path_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar o gráfico para visualização dos dados\n",
    "def gerarGraficos(df_bin):\n",
    "    g = sns.PairGrid(df_bin, vars=['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9'],\n",
    "                     hue='C', palette='RdBu_r')\n",
    "    g.map(plt.scatter, alpha=0.9)\n",
    "    g.add_legend();\n",
    "    \n",
    "    h = sns.lmplot('A1', 'A2', col='C', hue='C', data=df_bin,\n",
    "                   markers=\".\",  scatter_kws=dict(color='blue'))\n",
    "    i = sns.lmplot('A3', 'A4', col='C', data=df_bin,\n",
    "                   markers=\".\", scatter_kws=dict(color='orange'))\n",
    "    j = sns.lmplot('A6', 'A6', col='C', data=df_bin,\n",
    "                   markers=\".\", scatter_kws=dict(color='yellow'))\n",
    "    k = sns.lmplot('A7', 'A8', col='C', data=df_bin,\n",
    "                   markers=\".\", scatter_kws=dict(color='black'))\n",
    "    l = sns.lmplot('A9', 'A1', col='C', data=df_bin,\n",
    "                   markers=\".\", scatter_kws=dict(color='green'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para transformação da coluna de Classes: Converter para abordagem binária:\n",
    "# (Classes 2 (amarela) e 3 (marrom) - As duas tem a doença: deixar como classe '2')\n",
    "def column_transform(df_bin,path_out,contador_pastas,filepaths_1):\n",
    "    df_bin['C'] = df_bin['C'].replace([2, 3], int(2))\n",
    "\n",
    "    # A Classe 1 (verde) - Não tem a doença: deixar como classe '0')\n",
    "    df_bin['C'] = df_bin['C'].replace([1],int(0))\n",
    "\n",
    "    # Alterar de Classe 2 para '1' para padronização (1 Tem a doença e 0 não tem a doença)\n",
    "    df_bin['C'] = df_bin['C'].replace([2],int(1))\n",
    "    \n",
    "    # Normalizando o dataframe.\n",
    "    #df_bin= (df_bin-df_bin.min())/(df_bin.max()-df_bin.min())\n",
    "\n",
    "    # Gravar os concatenados no dataframe (com as informações alteradas para o contexto binário) em disco\n",
    "    df_bin.to_csv(path_out + 'dados_PCA_binaria_' + \\\n",
    "                  filepaths_1[contador_pastas] + '_.csv', index = None, header=True)\n",
    "    # Function call Upload files function from workspace files to Selected Bucket\n",
    "    fun.upload_from_bucket('BK-classification', url, path_out + 'dados_PCA_binaria_' + \\\n",
    "                  filepaths_1[contador_pastas] + '_.csv')\n",
    "    \n",
    "    return df_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 1671,
     "status": "ok",
     "timestamp": 1608133339485,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "3KSXFfnLizoI",
    "outputId": "558be32d-537b-4189-f1bb-25a3d194fcbf"
   },
   "outputs": [],
   "source": [
    "# Função para verificação de dados faltantes no dataset\n",
    "def dadosFaltando():\n",
    "    #Verificar e imprimir os valores 'Nah' no dataset  \n",
    "    print(df_bin.isnull().sum(axis=0))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 11583,
     "status": "ok",
     "timestamp": 1608133349433,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "y5UBh-LAJXSK",
    "outputId": "13e1a961-a925-44d8-cd4f-d1aaacc48c32"
   },
   "outputs": [],
   "source": [
    "# Função para definição das colunas e divisão do dataframe para treino e teste\n",
    "def colunms_dataframe_division():\n",
    "    features = ['A0','A1','A2','A3','A4','A5','A6','A7','A8','A9']\n",
    "    target = ['C']\n",
    "    #X = df_bin[features].to_numpy() alterado\n",
    "    X = df_bin[features]\n",
    "    #y = df_bin[target].to_numpy() alterado\n",
    "    y = df_bin[target]\n",
    "\n",
    "    x_tr, x_te, y_tr, y_te = train_test_split(X, y,\n",
    "                                                test_size = teste,\n",
    "                                                random_state=42, stratify=y)\n",
    "                                                #stratify=y (mantem a mesma proporção das classes nas divisões)\n",
    "    print(x_tr.shape, x_te.shape)\n",
    "    \n",
    "    return x_tr, x_te, y_tr, y_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 939115,
     "status": "ok",
     "timestamp": 1608134277002,
     "user": {
      "displayName": "Ricardo Alexandre Neves",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgsTSa-W1yyegcSta3R3jtUTo9ySHtV89RG_o-Txg=s64",
      "userId": "08355791563874562324"
     },
     "user_tz": 180
    },
    "id": "uh0dVQxGz5G-",
    "outputId": "da024073-61cb-4113-8a68-12b5a30ed25b"
   },
   "outputs": [],
   "source": [
    "# Cálculo de Estimativas para o Classificador (Busca dos melhores parâmetros)\n",
    "\n",
    "def BaggingClassifierTest(C,gamma,):\n",
    "    #Aplicação do algoritmo SVM - Binário (Teste de BaggingClassifier)\n",
    "    n_estimators = 10\n",
    "\n",
    "    # Teste de instrução\n",
    "    clf_svm = BaggingClassifier(SVC(kernel='poly', degree= 7, probability=True,\n",
    "                                   class_weight={0: 0.1, 1: 0.9}, C=C, gamma=gamma), max_samples=10.0 /\n",
    "                                   n_estimators, bootstrap=True, n_estimators=n_estimators, n_jobs=-1)\n",
    "    return clf_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinamento e teste do modelo no classificador SVM (uso de estimativas Gridsearch)\n",
    "def training_testing_SVM(x_tr, x_te, y_tr, y_te):\n",
    "    tr_std_svm = []\n",
    "  \n",
    "    # Escalar os dados de Treino e Teste\n",
    "    scaler_x_tr = StandardScaler().fit(x_tr)\n",
    "    x_tr_scaled = scaler_x_tr.transform(x_tr)\n",
    "    scaler_x_te = StandardScaler().fit(x_te)\n",
    "    x_te_scaled = scaler_x_te.transform(x_te)\n",
    "\n",
    "    # Chamada do Classificador (SVM)\n",
    "    # clf_svm = svm.SVC(kernel='poly', degree=7, probability=True, gamma=1, C=1, class_weight={0.0: 0.1, 1.0: 0.9})\n",
    "    clf_svm = BaggingClassifierTest(1,1)\n",
    "\n",
    "    clf_svm.fit(x_tr_scaled, y_tr.values.ravel()) \n",
    "    #clf_svm.fit(x_tr_scaled, y_tr) alterado\n",
    "    y_true3, y_pred3 = y_te, clf_svm.predict(x_te_scaled)\n",
    "    scores_svm = cross_val_score(clf_svm, x_tr_scaled, y_tr.values.ravel(), cv=10)\n",
    "    #scores_svm = cross_val_score(clf_svm, x_tr_scaled, y_tr, cv=10) alterado\n",
    "    tr_std_svm.append(np.std(scores_svm))\n",
    "    accsvm = accuracy_score(y_te, y_pred3)\n",
    "    mse = mean_squared_error(y_pred3, y_te)\n",
    "    report = classification_report(y_true3, y_pred3)\n",
    "    \n",
    "    return accsvm, tr_std_svm, mse, report, clf_svm, x_tr_scaled, x_te_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gerar o arquivo de dados resultantes do Classificador\n",
    "def data_file_generate(teste, accsvm, tr_std_svm, mse, report,filepaths_1,contador_pastas):\n",
    "    \n",
    "    # Variável Global para armazenamento das informações durante o processo de classificação\n",
    "    text =''\n",
    "    text+=\"\\n---------------------------------------------------------------------------------\\n\"\n",
    "    text+=\"Dados Classificador SVM (Binário) ---> \" + \"Imagem: \" + filepaths_1[contador_pastas]\n",
    "    text+=\"\\nPorcentagem de \" + str(100-(teste*100)) + \"% para Treinamento e \" + \\\n",
    "        str(teste*100) + \"% para Teste\"\n",
    "    text+=\"\\nAcurácia(SVM): \" + str(accsvm)\n",
    "    text+=\"\\nDesvio Padrão(SVM): \" + str(tr_std_svm)\n",
    "    text+=\"\\nMean Squared Error = \" + str(mse)\n",
    "    text+=\"\\n\\nRelatório de Classificação:\\n\" + str(report)\n",
    "    text+=\"\\n----------------------------------\\n\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Função para Plot da curva ROC - Modelos\n",
    "def plot_curva_ROC(titulo, fpr, tpr, path_out, contador_pastas, filepaths_1):\n",
    "  \n",
    "    plt.title(titulo)\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.plot([0, 1], ls=\"--\")\n",
    "    plt.plot([0, 0], [1, 0] , c=\".5\"), plt.plot([1, 1] , c=\".5\")\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "\n",
    "    plt.savefig(path_out + 'Curva_ROC_' + filepaths_1[contador_pastas] + '.png', format='png')\n",
    "    # Function call Upload files function from workspace files to Selected Bucket\n",
    "    fun.upload_from_bucket('BK-classification', url, path_out + 'Curva_ROC_' + filepaths_1[contador_pastas] + '.png')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Função para construção da curva ROC\n",
    "def ROC_curve_build(text, clf_svm, path_out, contador_pastas,filepaths_1):\n",
    "    # Cabeçalho das informações da Curva ROC\n",
    "    text+= \"Dados Curva ROC \\n\"\n",
    "\n",
    "    #Cálculos de Predição - SVM\n",
    "    probs = clf_svm.predict_proba(x_te_scaled)\n",
    "    probs = probs[:, 1]\n",
    "\n",
    "    # Calcular a Curva ROC\n",
    "    auc = roc_auc_score(y_te, probs)\n",
    "    fpr, tpr, thresholds = roc_curve(y_te, probs)\n",
    "\n",
    "    # Informação gerada do Cálculo da Curva ROC\n",
    "    text+='AUC(Area Under the Curve - SVM): %.2f\\n' % auc\n",
    "\n",
    "    # Chamada de função para plotagem da Curva ROC\n",
    "    plot_curva_ROC('Curva ROC - SVM',fpr,tpr,path_out,contador_pastas,filepaths_1)\n",
    "\n",
    "    # Finalização do cabeçalho\n",
    "    text+= \"---------------------------------------------------------------------------------\\n\"\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Matrix de Confusão\n",
    "\n",
    "def plot_matrix(y_tr, y_tr_pred, title, path_out,contador_pastas,filepaths_1):\n",
    "    cm = confusion_matrix(y_tr, y_tr_pred, normalize=None)\n",
    "    cmd = ConfusionMatrixDisplay(cm, display_labels=['Negativo','Positivo'])\n",
    "    cmd.plot(values_format='d')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Predito')\n",
    "    plt.ylabel('Real')\n",
    "    \n",
    "    plt.savefig(path_out + 'Matriz_Confusão_' + filepaths_1[contador_pastas] +\\\n",
    "            '.png', format='png')\n",
    "    # Function call Upload files function from workspace files to Selected Bucket\n",
    "    fun.upload_from_bucket('BK-classification', url, path_out + 'Matriz_Confusão_' +\\\n",
    "                            filepaths_1[contador_pastas] + '.png')\n",
    "          \n",
    "    plt.show()\n",
    "\n",
    "# Função para construir a matriz de confusão\n",
    "def confusion_matrix_build(path_out,contador_pastas,filepaths_1):\n",
    "    y_tr_pred3 = cross_val_predict(clf_svm, x_tr, y_tr.values.ravel(), cv=5)\n",
    "    #y_tr_pred3 = cross_val_predict(clf_svm, x_tr, y_tr, cv=5) alterado\n",
    "    plot_matrix(y_tr, y_tr_pred3, 'Matriz - SVM', path_out, contador_pastas, filepaths_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para gravar a String de informações do processo em arquivo\n",
    "def file_save_string(filepaths_1, contador_pastas, text, path_out):\n",
    "    \n",
    "    with open(path_out + 'analise_classificadores_' + filepaths_1[contador_pastas] + \\\n",
    "              '_'+ str(100-(teste*100)) + '_Treino_' + str(teste*100) + '_Teste.txt', 'w') as f:\n",
    "        f.write(text)\n",
    "       \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para gravar documento (formato .docx) contendo o relatório e as imagens\n",
    "def document_save_docx(text,path_out,contador_pastas,filepaths_1):\n",
    "    # Matriz de Confusão e Curva ROC\n",
    "    document = Document()\n",
    "    p = document.add_paragraph(text)\n",
    "    r = p.add_run()\n",
    "   \n",
    "    r.add_picture(path_out + 'Curva_ROC_' + filepaths_1[contador_pastas] + \\\n",
    "                  '.png',width=Inches(3.5), height=Inches(2.7))\n",
    "    r.add_picture(path_out + 'Matriz_Confusão_' + filepaths_1[contador_pastas] + \\\n",
    "                  '.png',width=Inches(2.5), height=Inches(1.7))\n",
    "    \n",
    "    r.add_text('---------------------------------------------------------------------------------')\n",
    "\n",
    "    document.save(path_out + 'analise_classificador_SVM_binario' + \\\n",
    "                  filepaths_1[contador_pastas] + '_'+ str(teste*100) + \\\n",
    "                  '_Teste_' + str(100-(teste*100)) + '_Treino.docx')\n",
    "\n",
    "    # Function call Upload files function from workspace files to Selected Bucket\n",
    "    fun.upload_from_bucket('BK-classification', url, path_out + 'analise_classificador_SVM_binario' + \\\n",
    "                  filepaths_1[contador_pastas] + '_'+ str(teste*100) + \\\n",
    "                  '_Teste_' + str(100-(teste*100)) + '_Treino.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuração para Teste de Novos Dados de Entrada\n",
    "\n",
    "def novosDados(contador_pastas,clf_svm):\n",
    "    \n",
    "    # Execução de loop de acordo com a quantidade de imagens para processamento\n",
    "    for i in range(0,53): # range = quantidade de pastas disponíveis no bucket 'BK-input-data-classification'\n",
    "    \n",
    "        # Chamada de função para obter objetos selecionados do bucket que contém\n",
    "        #    os dados de todas as imagens separadas em pastas individuais\n",
    "        message, etapa = objects_to_bucket(1,'s', 'BK-input-data-classification', resource_value, url, path_2)\n",
    "        print(message)\n",
    "\n",
    "        # Chamada de função para estruturação de arquivos\n",
    "        # Configuração dos parâmetros de estruturas de pastas e buckets para entradas de Novos Dados\n",
    "        df_bin, filepaths_1, path_out = files_structure(contador_pastas, path_2)\n",
    "\n",
    "        # Chamada de função para transformação de colunas\n",
    "        df_bin = column_transform(df_bin,path_out,contador_pastas,filepaths_1)\n",
    "\n",
    "        # Visualizar dados faltantes 'Missing values'\n",
    "        dadosFaltando()\n",
    "\n",
    "        ###-------------------------------------------------------------------------------------\n",
    "        # A partir do objeto do Classificador SVM (já Treinado com os dados de todas as imagem)\n",
    "        text_dados=''\n",
    "        text_dados+= \"Dados da Imagem: \" + str(filepaths_1[contador_pastas]) + \"\\n\\n\"\n",
    "        # Ler o arquivo da imagem atual\n",
    "        df_atual = pd.DataFrame()\n",
    "        df_atual = pd.read_csv(path_out + 'dados_PCA_' + filepaths_1[contador_pastas] + '_.csv')\n",
    "        \n",
    "        # Retirar a coluna 'C' do Dataframe que corresponde ao \"target\" no processo de\n",
    "        #   divisão dos dados de Treino e Teste do Classificador\n",
    "        # Conversão de Dataframe para Array Numpy (intensão de mandar um bloco)\n",
    "        df_atual = df_atual.drop(columns=['C']).to_numpy()\n",
    "        \n",
    "        # Escalar o valor do conjunto de dados de cada imagem\n",
    "        scaler_img = StandardScaler().fit(df_atual)\n",
    "        df_atual_scaled = scaler_img.transform(df_atual)\n",
    "        \n",
    "        # Predizer o valor de cada imagem\n",
    "        new_y_pred = clf_svm.predict(df_atual_scaled)\n",
    "        text_dados+=(\"Valor da Predição: \\n\" + str(new_y_pred))\n",
    "        text_dados+=\"\\n\\n--------------------------------------------------------------\"\n",
    "        \n",
    "        # Salvar o resultado da Predição de cada imagem\n",
    "        file_save_string(filepaths_1, contador_pastas, text_dados, path_out)\n",
    "        \n",
    "        # Incremento do contador de pastas\n",
    "        contador_pastas+=1\n",
    "          \n",
    "    return df_atual, df_atual_scaled, new_y_pred, text_dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada para execução do Processo de Treino e Teste do Classificador\n",
    "    \n",
    "# Chamada de função para estruturação de arquivos\n",
    "df_bin, filepaths_1, path_out = files_structure(contador_pastas, path_1)\n",
    "\n",
    "# Chamada de função para transformação de colunas\n",
    "df_bin = column_transform(df_bin,path_out,contador_pastas,filepaths_1)\n",
    "\n",
    "# Visualizar dados faltantes 'Missing values'\n",
    "dadosFaltando()\n",
    "\n",
    "#Chamada de função para definição das colunas e divisão do dataframe para treino e teste\n",
    "x_tr, x_te, y_tr, y_te = colunms_dataframe_division()\n",
    "\n",
    "# Chamada de função treino e teste do modelo no classificador SVM\n",
    "accsvm, tr_std_svm, mse, report, clf_svm, x_tr_scaled, x_te_scaled = training_testing_SVM(x_tr, x_te, y_tr, y_te)\n",
    "\n",
    "# Chamada de função para gerar o arquivo de dados resultantes do Classificador\n",
    "text = data_file_generate(teste, accsvm, tr_std_svm, mse, report,filepaths_1, contador_pastas)\n",
    "\n",
    "# Chamada de função para construção da curva ROC\n",
    "text = ROC_curve_build(text, clf_svm, path_out, contador_pastas, filepaths_1)\n",
    "\n",
    "# Chamada de função para construir a matriz de confusão\n",
    "#confusion_matrix_build(path_out,contador_pastas,filepaths_1)\n",
    "\n",
    "# Chamada de função para gravar a String de informações do processo em arquivo\n",
    "text = file_save_string(filepaths_1, contador_pastas, text, path_out)\n",
    "\n",
    "# Chamada de função para gravar documento .docx\n",
    "#document_save_docx(text,path_out,contador_pastas,filepaths_1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "## Processo de entrada de Novos Dados\n",
    "\n",
    "# Chamada da Função para Teste de Novos Dados\n",
    "contador_pastas=0\n",
    "#df_atual, df_atual_scaled, new_y_pred, text_dados = novosDados(contador_pastas, clf_svm)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Algoritmo_V5.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "e31a0e611b8724fca1d58a7ef46067b50a018b39ecc580ed645151c3e5f1bc5d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
